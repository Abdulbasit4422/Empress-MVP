
import os
from typing import List, Dict, Any, Optional
import time
import subprocess

from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv, find_dotenv


load_dotenv(find_dotenv("rag_env.env"))

# --- Configuration for Pinecone and Google Generative AI ---
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")

if not PINECONE_API_KEY: # PINECONE_ENVIRONMENT is deprecated for new client
    raise ValueError("PINECONE_API_KEY must be set in environment variables.")
if not GOOGLE_API_KEY:
    raise ValueError("GOOGLE_API_KEY or GEMINI_API_KEY must be set in environment variables.")

# Initialize Pinecone client
pinecone_client = Pinecone(api_key=PINECONE_API_KEY)

# --- Helper function to delete Pinecone index ---
def delete_pinecone_index(index_name: str = "rag-knowledge-base"):
    """
    Deletes a Pinecone index if it exists.
    """
    print(f"Attempting to delete Pinecone index: {index_name}")
    if index_name in pinecone_client.list_indexes():
        pinecone_client.delete_index(index_name)
        print(f"Pinecone index \'{index_name}\' deleted successfully. Waiting for 10 seconds...")
        time.sleep(10) # Give Pinecone time to process deletion
    else:
        print(f"Pinecone index \'{index_name}\' does not exist.")

# Initialize Google Generative AI Embeddings
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=GOOGLE_API_KEY)

# --- 1. Data Ingestion Module ---

def ingest_data(
    file_path: str,
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
    metadata: Optional[Dict[str, Any]] = None,
) -> List[Document]:
    """
    Loads and preprocesses data from a given file path, supporting various formats.
    Splits the document into smaller, semantically meaningful chunks.

    Args:
        file_path (str): The path to the document file (e.g., .pdf, .txt).
        chunk_size (int): The maximum size of each text chunk.
        chunk_overlap (int): The number of characters to overlap between chunks.
        metadata (Optional[Dict[str, Any]]): Additional metadata to attach to each document chunk.

    Returns:
        List[Document]: A list of LangChain Document objects, each representing a chunk
                        of the ingested data with associated metadata.
    """
    print(f"Ingesting data from: {file_path}")
    
    # Determine loader based on file extension
    file_extension = os.path.splitext(file_path)[1].lower()
    
    if file_extension == ".pdf":
        loader = PyPDFLoader(file_path)
    elif file_extension == ".txt":
        loader = TextLoader(file_path)
    # Add more loaders for other document types as needed (e.g., .docx, .md)
    else:
        raise ValueError(f"Unsupported file type: {file_extension}")

    # Load documents
    documents = loader.load()
    print(f"Loaded {len(documents)} raw documents/pages.")

    # Split documents into chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        is_separator_regex=False,
    )
    chunks = text_splitter.split_documents(documents)
    print(f"Split into {len(chunks)} chunks.")

    # Add additional metadata if provided
    if metadata:
        for chunk in chunks:
            chunk.metadata.update(metadata)
            
    return chunks

# --- 2. Embedding & Storage Module ---

def embed_and_store_data(
    chunks: List[Document],
    index_name: str = "rag-knowledge-base",
    embedding_dimension: int = 768, # Default for models/embedding-001
    metric: str = "cosine",
) -> PineconeVectorStore:
    """
    Generates embeddings for document chunks and stores them in a Pinecone index.

    Args:
        chunks (List[Document]): A list of LangChain Document objects to embed and store.
        index_name (str): The name of the Pinecone index.
        embedding_dimension (int): The dimension of the embeddings generated by the model.
        metric (str): The similarity metric for the Pinecone index (e.g., \'cosine\', \'euclidean\').

    Returns:
        PineconeVectorStore: An initialized PineconeVectorStore object connected to the index.
    """
    print(f"Embedding and storing {len(chunks)} chunks into Pinecone index: {index_name}")
    # Check if index exists
    existing_indexes = pinecone_client.list_indexes()
    existing_index_names = [index.name for index in existing_indexes.indexes]

    if index_name not in existing_index_names:
        print(f"Creating new Pinecone index: {index_name}")
        pinecone_client.create_index(
            name=index_name,
            dimension=embedding_dimension,
            metric=metric,
            spec=ServerlessSpec(cloud='aws', region='us-east-1')
        )
        print(f"Index {index_name} created. Waiting for index to be ready...")
        time.sleep(60) # Wait for index to be ready
    else:
        print(f"Using existing Pinecone index: {index_name}")    # Store documents in Pinecone
    vectorstore = PineconeVectorStore.from_documents(
        documents=chunks,
        embedding=embeddings,
        index_name=index_name,
    )
    print(f"Successfully stored {len(chunks)} chunks in Pinecone index {index_name}.")
    return vectorstore

# --- 3. Retrieval Module ---

def retrieve_documents(
    query: str,
    vectorstore: PineconeVectorStore,
    top_k: int = 5,
    filters: Optional[Dict[str, Any]] = None,
) -> List[Document]:
    """
    Queries the Pinecone vector store to fetch the most relevant document chunks.

    Args:
        query (str): The user\'s query string.
        vectorstore (PineconeVectorStore): The initialized PineconeVectorStore object.
        top_k (int): The number of top relevant documents to retrieve.
        filters (Optional[Dict[str, Any]]): Metadata filters to apply during retrieval.

    Returns:
        List[Document]: A list of LangChain Document objects representing the retrieved chunks.
    """
    print(f"Retrieving top {top_k} documents for query: \033[1m\'{query}\'\033[0m")
    
    # Perform similarity search with optional filters
    if filters:
        retrieved_docs = vectorstore.similarity_search(query, k=top_k, filter=filters)
    else:
        retrieved_docs = vectorstore.similarity_search(query, k=top_k)
        
    print(f"Retrieved {len(retrieved_docs)} documents.")
    return retrieved_docs

# --- 4. Augmented Generation Module ---

def augment_and_generate_response(
    query: str,
    retrieved_documents: list,
    llm_model_name: str = "gemini-2.5-flash",
    system_prompt: str | None = None,
) -> str:
    """
    Passes retrieved chunks to an LLM (Google Generative AI) for context-aware answers.

    Args:
        query (str): The original user query.
        retrieved_documents (List[Document]): A list of LangChain Document objects retrieved from the vector store.
        llm_model_name (str): The name of the Google Generative AI LLM to use (e.g., "gemini-pro").
        system_prompt (str, optional): The system prompt to use. Defaults to a generic prompt.

    Returns:
        str: The context-aware response generated by the LLM.
    """
    print(f"Augmenting and generating response for query: \033[1m'{query}'\033[0m")

    # Initialize the LLM
    llm = ChatGoogleGenerativeAI(model=llm_model_name, google_api_key=GOOGLE_API_KEY)

    # Use the provided system_prompt or a default one
    if system_prompt is None:
        system_prompt = """
        You are Ask Empress, a trusted Peri+Menopausal Health and Wellness Expert. 
Your role is to provide users with clear, empathetic, and deeply informative answers to their questions. 

When responding:
1. **Be comprehensive and well-structured** â€“ organize your response into clear sections, You can use the format below when you think it is best, but can also be dynamic by adopting better ones when need arise, be dynamic to suite each question
   - Overview
   - Causes or Contributing Factors 
   - Management & Lifestyle Recommendations  
   - When to Seek Professional Help (if relevant).
   - Disclaimer to consult a doctor when necessary.

these format above are just to guide you, you can always adjust it as the case may be, and use what suite the questions the most.

2. Ground your advice in the retrieved knowledge base as much as possible. If no relevant information is available, rely on your medical expertise but be transparent about it.

3. Personalize your response to the userâ€™s concern, showing empathy and reassurance in a compassionate tone. 

4. Avoid short or generic answers â€“ aim for depth, clarity, elaborate, comprehensive, well structured and practical guidance, with good fonts and relevants emojis. 
"""

    # Create a prompt template
    template = f"""{system_prompt}

    Context:
    {{context}}

    Question: {{question}}

    Answer:
    """
    prompt = ChatPromptTemplate.from_template(template)

    # Format the context from retrieved documents
    context_text = "\n\n".join([doc.page_content for doc in retrieved_documents])

    # Create the RAG chain
    rag_chain = (prompt | llm | StrOutputParser())

    # Invoke the chain
    response = rag_chain.invoke({"context": context_text, "question": query})

    print("Successfully generated response.")
    return response


# --- Main RAG Pipeline Orchestration ---

def run_rag_pipeline(
    file_paths: List[str],
    user_query: str,
    index_name: str = "rag-knowledge-base",
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
    top_k_retrieval: int = 5,
    retrieval_filters: Optional[Dict[str, Any]] = None,
    llm_model_name: str = "gemini-2.5-flash",
) -> Dict[str, Any]:
    """
    Orchestrates the complete RAG pipeline from data ingestion to augmented generation.

    Args:
        file_paths (List[str]): List of paths to document files for ingestion.
        user_query (str): The user\'s query for the RAG system.
        index_name (str): Name of the Pinecone index.
        chunk_size (int): Size of text chunks.
        chunk_overlap (int): Overlap between text chunks.
        top_k_retrieval (int): Number of top documents to retrieve.
        retrieval_filters (Optional[Dict[str, Any]]): Filters for Pinecone retrieval.
        llm_model_name (str): Name of the LLM for generation.

    Returns:
        Dict[str, Any]: A dictionary containing the final response and retrieved documents.
    """
    print("\n--- Running Full RAG Pipeline ---")
    all_chunks = []
    for fp in file_paths:
        # Assuming basic metadata for demonstration; enhance as needed
        file_metadata = {"source": os.path.basename(fp), "category": "general_info"}
        if "pdf" in fp.lower():
            file_metadata["category"] = "medical_report"
        elif "txt" in fp.lower():
            file_metadata["category"] = "doctor_info"

        chunks = ingest_data(fp, chunk_size, chunk_overlap, file_metadata)
        all_chunks.extend(chunks)

    if not all_chunks:
        return {"response": "No data ingested.", "retrieved_documents": []}

    vectorstore = embed_and_store_data(all_chunks, index_name)

    retrieved_docs = retrieve_documents(user_query, vectorstore, top_k_retrieval, retrieval_filters)

    if not retrieved_docs:
        return {"response": "No relevant documents found.", "retrieved_documents": []}

    final_response = augment_and_generate_response(user_query, retrieved_docs, llm_model_name)

    return {"response": final_response, "retrieved_documents": retrieved_docs}


if __name__ == "__main__":
    try:
        # Clean up any existing index before running the example
        delete_pinecone_index("empress")
        print("\n--- Running Full RAG Pipeline Example ---")
        query = "What are the symptoms of a migraine and who treats heart conditions?"
        file_paths_to_ingest = ["Empress_merged.pdf"]
        
        rag_output = run_rag_pipeline(
            file_paths=file_paths_to_ingest,
            user_query=query,
            index_name="empress",
            top_k_retrieval=10, 
        )

        print("\n--- Final RAG Pipeline Output ---")
        print("Response: " + rag_output["response"])
        print("\nRetrieved Documents (Metadata Only):")
        for i, doc in enumerate(rag_output["retrieved_documents"]):
            print("  {}. Content: {}..., Source: {}, Category: {}".format(i+1, doc.page_content[:100], doc.metadata.get("source"), doc.metadata.get("category")))

    except ValueError as ve: # Original except block
        print(f"Configuration Error: {ve}")
    except subprocess.CalledProcessError as cpe:
        print(f"Error during PDF conversion: {cpe.stderr}")
    except Exception as e:
        print(f"An error occurred during the full RAG pipeline example: {e}")

    finally:
        # Optional: Delete Pinecone index after testing
        # try:
        #     if "empress" in pinecone_client.list_indexes():
        #         pinecone_client.delete_index("empress")
        #         print("Pinecone index \\\'empress\\\' deleted.")
        # except Exception as e:
        #     print(f"Error deleting Pinecone index: {e}")
        pass # This comment is to ensure the finally block is not empty

import json
import codecs
from typing import Any

import re

def clean_output(raw_response: str) -> str:
    """
    Cleans the raw LLM response by removing Markdown formatting (#, *, -) 
    and newlines, leaving only plain text.
    """
    # Remove leading/trailing whitespace from each line
    lines = [line.strip() for line in raw_response.split('\n')]

    # Remove multiple consecutive blank lines
    cleaned_lines = []
    for line in lines:
        if line or (cleaned_lines and cleaned_lines[-1] != ''):
            cleaned_lines.append(line)
    cleaned_response = '\n'.join(cleaned_lines)

    # Replace multiple spaces with a single space
    cleaned_response = re.sub(r'\s+', ' ', cleaned_response)

    # ðŸ”´ Remove Markdown symbols (#, *, -)
    cleaned_response = re.sub(r'[#!*\-\n\n####Ã°ÂŸÂŒÂ¬Ã¯Â¸ÂÂ¶Ã¢]', '', cleaned_response)

    # ðŸ”´ Remove all newline characters (\n, \n\n, etc.)
    cleaned_response = cleaned_response.replace('\n', ' ')

    # Final trim
    cleaned_response = cleaned_response.strip()

    return cleaned_response





# --- Q&A Chatbot Functionality ---
# Ensure clean_output is defined once (above) and reused
# import json, codecs already included earlier

def chatbot_qa(query: str, index_name: str = "empress", system_prompt: Optional[str] = None) -> Dict[str, Any]:
    """
    You are Ask Empress, a trusted Peri+Menopausal Health and Wellness Expert. 
Your role is to provide users with clear, empathetic, and deeply informative answers to their questions. 

When responding:
1. **Be comprehensive and well-structured** â€“ organize your response into clear sections, You can use the format below when you think it is best, but can also be dynamic by adopting better ones when need arise, be dynamic to suite each question
   - Overview
   - Causes or Contributing Factors 
   - Management & Lifestyle Recommendations  
   - When to Seek Professional Help (if relevant).
   - Disclaimer to consult a doctor when necessary.

these format above are just to guide you, you can always adjust it as the case may be, and use what suite the questions the most.

2. Ground your advice in the retrieved knowledge base as much as possible. If no relevant information is available, rely on your medical expertise but be transparent about it.

3. Personalize your response to the userâ€™s concern, showing empathy and reassurance in a compassionate tone. 

4. Avoid short or generic answers â€“ aim for depth, clarity, elaborate, comprehensive, well structured and practical guidance, with good fonts and relevants emojis. 


    Args:
        query (str): The user's question.
        index_name (str): The name of the Pinecone index to query.

    Returns:
        Dict[str, Any]: A dictionary containing the LLM's response and retrieved documents.
    """    
    print(f"\n--- Q&A Chatbot: Processing query \'{query}\' ---")
    # For Q&A, we don't need to ingest new data, just retrieve and generate
    # We will assume the index is already populated with Empress_merged.pdf

    # Initialize vectorstore for retrieval
    vectorstore = PineconeVectorStore.from_existing_index(
        index_name=index_name,
        embedding=embeddings,
    )



    # Craft a query to find doctors related to the symptoms
    default_system_prompt = f"You are Ask Empress, a trusted Peri+Menopausal Health and Wellness Expert who provides clear, empathetic, deeply informative answers that are comprehensive, well-structured (overview, causes, management, when to seek help, disclaimer), grounded in retrieved knowledge or transparent expertise, personalized with compassion, and never short or generic but elaborate, practical, and engaging with clarity, good structure, and relevant emojis" 
    chosen_system_prompt = system_prompt or default_system_prompt
    
    retrieved_docs = retrieve_documents(query, vectorstore, top_k=10)

    if not retrieved_docs:
        return {"response": "I am a peri+menopausal Health and Wellness Expert, Kindly ask question within my context .", "retrieved_documents": []}
    
    
    raw = augment_and_generate_response(query, retrieved_docs, llm_model_name="gemini-2.5-flash", system_prompt=chosen_system_prompt )
    cleaned_response = clean_output(raw)

    return {"response": cleaned_response, "retrieved_documents": retrieved_docs}




# --- Doctor Symptoms Matching Functionality ---
def doctor_symptoms_matching(symptoms: str, index_name: str = "empress") -> Dict[str, Any]:
    """
    Matches patient symptoms to doctors based on the knowledge base, and return the name of most befitting Doctor you ranked that best match the symptoms provided.

    Args:
        symptoms (str): A string of patient symptoms (e.g., "fever, cough, headache").
        index_name (str): The name of the Pinecone index to query.

    Returns:
        Dict[str, Any]: A dictionary containing the LLM's response with doctor recommendations and retrieved documents.
    """
    print(f"\n--- Doctor Symptoms Matching: Processing symptoms \'{symptoms}\' ---")

    # Initialize vectorstore for retrieval
    vectorstore = PineconeVectorStore.from_existing_index(
        index_name=index_name,
        embedding=embeddings,
    )

    # Craft a query to find doctors related to the symptoms
    query = f"You are a helpful Medical triage assistant that matches doctors to patient symptoms based on the provided context, Matches patient symptoms {symptoms} to doctors based on the knowledge base, and return the name of most befitting Doctor you ranked that best match the symptoms provided. If no specific doctor is mentioned for the symptoms, state that based on the provided information."
    retrieved_docs = retrieve_documents(query, vectorstore, top_k=10)

    if not retrieved_docs:
        return {"response": "No doctors found for the given symptoms in the knowledge base.", "retrieved_documents": []}

    # Craft a specific prompt for doctor matching
    prompt_template = """
    You are a helpful Medical triage assistant that matches doctors to patient symptoms based on the provided context.
    Matches patient symptoms to doctors based on the knowledge base, and return ONLY the name of most befitting Doctor you ranked that best match the symptoms provided. ONLY DOCTOR'S NAME 
    If no specific doctor is mentioned for the symptoms, suggest doctor you feel will best qualified to treat the symptoms based on your own intuition and ranking, never give a negative answer even if you didnt see any doctor that matches perfectly, suggest the one that is closest in qualification in treating the symptoms provided .

    Context:
    {context}

    Symptoms: {symptoms}

    Doctor Recommendations:
    """
    
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=GOOGLE_API_KEY)
    prompt = ChatPromptTemplate.from_template(prompt_template)
    context_text = "\n\n".join([doc.page_content for doc in retrieved_docs])
    rag_chain = (prompt | llm | StrOutputParser())
    
    response = rag_chain.invoke({"context": context_text, "symptoms": symptoms})

    print("Successfully generated doctor recommendations.")
    return {"response": response, "retrieved_documents": retrieved_docs}





# --- Affirmation Recommendation Functionality ---
def affirmation_recommendation(categories: List[str], index_name: str = "empress") -> Dict[str, Any]:
    """
    Suggests 3 affirmations at random based on each chosen categories from the PDF knowledge base, that is, a random affirmation from each category.

    Args:
        categories (List[str]): A list of affirmation categories chosen by the patient.
        index_name (str): The name of the Pinecone index to query.

    Returns:
        Dict[str, Any]: A dictionary containing 3 random affirmations and retrieved documents.
    """
    print(f"\n--- Affirmation Recommendation: Processing categories {categories} ---")

    # Initialize vectorstore for retrieval
    vectorstore = PineconeVectorStore.from_existing_index(
        index_name=index_name,
        embedding=embeddings,
    )

    # Craft a query to find affirmations related to the categories
    query = f"Provide positive affirmations related to the following categories: {', '.join(categories)}. List several affirmations for each category."
    retrieved_docs = retrieve_documents(query, vectorstore, top_k=15) # Retrieve more docs to ensure enough affirmations

    if not retrieved_docs:
        return {"response": "No affirmations found for the given categories in the knowledge base.", "affirmations": [], "retrieved_documents": []}

    # Use LLM to extract and select affirmations
    prompt_template = """
    You are an expert assistant that extracts positive affirmations from the provided context.
    From the context,Suggests 3 affirmations at random based on each chosen categories from the PDF knowledge base, that is, a random affirmation from each category.: {categories}.
    Ensure each affirmation is on a new line. Do not include any other text or numbering.

    Context:
    {context}

    Affirmations:
    """
    
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=GOOGLE_API_KEY)
    prompt = ChatPromptTemplate.from_template(prompt_template)
    context_text = "\n\n".join([doc.page_content for doc in retrieved_docs])
    rag_chain = (prompt | llm | StrOutputParser())
    
    extracted_affirmations_str = rag_chain.invoke({"context": context_text, "categories": ', '.join(categories)})
    
    # Split the extracted string into a list of affirmations and clean them up
    extracted_affirmations = [aff.strip() for aff in extracted_affirmations_str.split('\n') if aff.strip()]
    
    # Select 3 random affirmations if available
    import random
    if len(extracted_affirmations) > 3:
        selected_affirmations = random.sample(extracted_affirmations, 3)
    else:
        selected_affirmations = extracted_affirmations

    print("Successfully generated affirmation recommendations.")
    return {"response": "Here are some affirmations for you:", "affirmations": selected_affirmations, "retrieved_documents": retrieved_docs}





# --- Product Recommendation Functionality ---
def product_recommendation(user_input: str, index_name: str = "empress") -> Dict[str, Any]:
    """
    Recommends products relevant to user input (symptoms or needs) by querying the PDF knowledge base, ensure the best matched products based on the symptoms provided are returned.

    Args:
        user_input (str): User's symptoms or product needs.
        index_name (str): The name of the Pinecone index to query.

    Returns:
        Dict[str, Any]: A dictionary containing the LLM's product recommendations and retrieved documents.
    """
    print(f"\n--- Product Recommendation: Processing user input \'{user_input}\' ---")

    # Initialize vectorstore for retrieval
    vectorstore = PineconeVectorStore.from_existing_index(
        index_name=index_name,
        embedding=embeddings,
    )

    # Craft a query to find products related to the user's input
    query = f"Recommend products that address or are related to: {user_input} by querying the PDF knowledge base, ensure the best matched products based on the symptoms provided are returned. Provide product names and a brief description."
    retrieved_docs = retrieve_documents(query, vectorstore, top_k=10)

    if not retrieved_docs:
        return {"response": "No relevant products found in the knowledge base for your input.", "products": [], "retrieved_documents": []}

    # Use LLM to extract and recommend products
    prompt_template = """
    You are a helpful assistant that recommends products based on the provided symptoms from context and user input.
    From the context, identify and list product names and their descriptions that are relevant to the user's needs and symptoms.
    List each product on a new line with its description. If no specific products are mentioned, provide them with the email and website links for further enquiry.

    Context:
    {context}

    User Input: {user_input}

    Product Recommendations:
    """
    
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=GOOGLE_API_KEY)
    prompt = ChatPromptTemplate.from_template(prompt_template)
    context_text = "\n\n".join([doc.page_content for doc in retrieved_docs])
    rag_chain = (prompt | llm | StrOutputParser())
    
    product_recommendations_str = rag_chain.invoke({"context": context_text, "user_input": user_input})
    
    # Split the extracted string into a list of products and clean them up
    product_list = [prod.strip() for prod in product_recommendations_str.split('\n') if prod.strip()]

    print("Successfully generated product recommendations.")
    return {"response": "Here are some product recommendations for you:", "products": product_list, "retrieved_documents": retrieved_docs}


